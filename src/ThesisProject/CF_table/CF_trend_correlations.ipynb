{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load the trends data from csv\n",
    "import pandas as pd\n",
    "\n",
    "trends = pd.read_csv('trends_raw.csv', header=None, names=['wf_id', 'trend'])\n",
    "#trends = pd.read_csv('trends.csv', header=1, names=['wf_id', 'trend', 'names']).drop(columns=['names'])\n",
    "\n",
    "# trends = trends.dropna() # there should not be any\n",
    "\n",
    "trends.describe()"
   ],
   "id": "5f2118822012bca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a mapping between wf_id and farm names\n",
    "import pandas as pd\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "\n",
    "# Initialize the database analyzer\n",
    "path = '/home/wheatley/WFD/wfd.db'  # Database path\n",
    "analyzer = DatabaseAnalyzer.WindFarmAnalyzer(path)\n",
    "\n",
    "# Create a dictionary to store wf_id to farm name mapping\n",
    "wf_id_to_name = {}\n",
    "\n",
    "# Get farm names for each wf_id in trends\n",
    "for wf_id in trends['wf_id']:\n",
    "    try:\n",
    "        farm_info = analyzer.get_farm_info(wf_id)\n",
    "        if farm_info:\n",
    "            farm_name = farm_info.get('plant_name')\n",
    "            if farm_name:\n",
    "                wf_id_to_name[wf_id] = farm_name\n",
    "            else:\n",
    "                # If no specific name field, try to construct from available fields\n",
    "                wf_id_to_name[wf_id] = f\"Farm_{wf_id}\"\n",
    "        else:\n",
    "            wf_id_to_name[wf_id] = f\"Unknown_Farm_{wf_id}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting farm info for wf_id {wf_id}: {e}\")\n",
    "        wf_id_to_name[wf_id] = f\"Error_Farm_{wf_id}\"\n",
    "\n",
    "# Add farm names to the trends DataFrame\n",
    "trends['farm_name'] = trends['wf_id'].map(wf_id_to_name)\n",
    "\n",
    "trends"
   ],
   "id": "df6ffb8fb937d251",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mean trend visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the mean of the trends\n",
    "mean_trend = np.mean(trends['trend'].values)\n",
    "# get the std of the trends\n",
    "std_trend = np.std(trends['trend'].values)\n",
    "# get the max of the trends\n",
    "\n",
    "# visualize the trends\n",
    "plt.figure(figsize=(20, 10), dpi=100)\n",
    "plt.axhline(y=mean_trend, color='black', linestyle='-', label=f'Mean = {mean_trend:.2f}')\n",
    "plt.axhline(y=mean_trend+std_trend, color='g', linestyle='--', linewidth=1, label=f'1 Std = {std_trend:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend, color='g', linestyle='--', linewidth=1) # No label needed if it's the same category\n",
    "plt.axhline(y=mean_trend+std_trend*2, color='y', linestyle='--', linewidth=3, label=f'2 Std = {std_trend*2:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend*2, color='y', linestyle='--', linewidth=3) # No label needed\n",
    "plt.axhline(y=mean_trend+std_trend*3, color='r', linestyle='--', linewidth=1, label=f'3 Std = {std_trend*3:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend*3, color='r', linestyle='--', linewidth=1) # No label needed\n",
    "plt.plot(trends['wf_id'], trends['trend'], 'o', markersize=8, label='Wind Farm Trends') # Added a label for the scatter plot\n",
    "plt.xlabel('Wind Farm ID')\n",
    "plt.ylabel('Trend')\n",
    "\n",
    "plt.legend(loc='upper left', ncol=1, frameon=True)\n",
    "\n",
    "plt.title('Wind Farm Trends')\n",
    "plt.show()"
   ],
   "id": "870ed993778efe54",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'trends' DataFrame is already defined and contains 'trend' column\n",
    "# Example 'trends' DataFrame for testing purposes:\n",
    "# np.random.seed(42)\n",
    "# trends = pd.DataFrame({'trend': np.random.normal(loc=50, scale=10, size=100)})\n",
    "# trends.loc[0, 'trend'] = 10 # Adding an outlier\n",
    "# trends.loc[1, 'trend'] = 90 # Adding another outlier\n",
    "\n",
    "# 1. Calculate Q1, Q3, and IQR for the 'trend' data\n",
    "Q1 = trends['trend'].quantile(0.25)\n",
    "Q3 = trends['trend'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "median = trends['trend'].median() # Calculate median\n",
    "mean = trends['trend'].mean() # Calculate mean\n",
    "\n",
    "# Calculate whisker values\n",
    "# Lower whisker is Q1 - 1.5*IQR or the lowest data point within that range\n",
    "lower_whisker = trends['trend'][trends['trend'] >= (Q1 - 1.5 * IQR)].min()\n",
    "# Upper whisker is Q3 + 1.5*IQR or the highest data point within that range\n",
    "upper_whisker = trends['trend'][trends['trend'] <= (Q3 + 1.5 * IQR)].max()\n",
    "\n",
    "print(f\"First Quartile (Q1): {Q1:.2f}\")\n",
    "print(f\"Third Quartile (Q3): {Q3:.2f}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR:.2f}\")\n",
    "print(f\"Median: {median:.2f}\") # Print median\n",
    "print(f\"Mean: {mean:.2f}\") # Print mean\n",
    "print(f\"Lower Whisker: {lower_whisker:.2f}\")\n",
    "print(f\"Upper Whisker: {upper_whisker:.2f}\")\n",
    "\n",
    "# 2. Create the figure and two subplots side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8), dpi=100, sharey=True) # sharey=True ensures same y-axis scale\n",
    "\n",
    "# Plot data points on the left subplot (axes[0])\n",
    "sns.stripplot(y=trends['trend'], jitter=0.2, color=\"darkblue\", alpha=0.7, ax=axes[0])\n",
    "axes[0].set_ylabel('Trend Value')\n",
    "axes[0].set_title('Individual Data Points')\n",
    "axes[0].set_xticks([]) # Remove x-axis tick labels for the stripplot\n",
    "axes[0].grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot box plot on the right subplot (axes[1])\n",
    "sns.boxplot(y=trends['trend'], color='lightblue', showmeans=True,\n",
    "            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"red\", \"markeredgecolor\":\"black\", \"markersize\":\"8\"},\n",
    "            medianprops={'color': 'orange', 'linewidth': 2},\n",
    "            ax=axes[1])\n",
    "axes[1].set_ylabel('') # No y-label for the second plot since y-axis is shared\n",
    "axes[1].set_title('Box Plot of Trends')\n",
    "axes[1].set_xticks([]) # Remove x-axis tick labels for the box plot\n",
    "axes[1].grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Overall title for the figure\n",
    "fig.suptitle('Individual Data Points & Box Plot', fontsize=16)\n",
    "\n",
    "# Create a custom legend for the box plot elements (mean, median, whiskers) and stripplot points\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements_box = [\n",
    "    Line2D([0], [0], color='orange', lw=2, label=f'Median: {median:.2f}'),\n",
    "    Line2D([0], [0], marker='o', color='w', label=f'Mean: {mean:.2f}',\n",
    "           markerfacecolor='red', markeredgecolor='black', markersize=8),\n",
    "    Line2D([0], [0], color='blue', lw=1, linestyle='--', label=f'Lower Whisker: {lower_whisker:.2f}'),\n",
    "    Line2D([0], [0], color='blue', lw=1, linestyle='--', label=f'Upper Whisker: {upper_whisker:.2f}'),\n",
    "]\n",
    "\n",
    "# You can place the legend on one of the subplots or outside the figure\n",
    "axes[1].legend(handles=legend_elements_box, loc='upper left', bbox_to_anchor=(0, 1)) # Place outside for clarity\n",
    "axes[0].legend(handles=[Line2D([0], [0], marker='o', color='darkblue', lw=0, label='Individual Data Points',\n",
    "           markerfacecolor='darkblue', markeredgecolor='darkblue', markersize=6)],\n",
    "               loc='upper left', bbox_to_anchor=(0, 1))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# 3. (Optional) Programmatically identify and list the outliers\n",
    "# These are already calculated as part of the whisker calculation\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = trends[(trends['trend'] < lower_bound) | (trends['trend'] > upper_bound)]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(\"\\nOutliers identified based on the 1.5xIQR rule:\")\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"\\nNo outliers found by the 1.5xIQR rule outside the whiskers.\")"
   ],
   "id": "f5d73670adc18829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove the outliers from the trends dataframe\n",
    "trends = trends[(trends['trend'] >= lower_bound) & (trends['trend'] <= upper_bound)]\n",
    "trends.to_csv('trends.csv', index=False)\n",
    "trends['trend'].describe().drop(labels=['count'])"
   ],
   "id": "c8a2b587ee638ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mean trend visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get the mean of the trends\n",
    "mean_trend = np.mean(trends['trend'].values)\n",
    "# get the std of the trends\n",
    "std_trend = np.std(trends['trend'].values)\n",
    "# get the max of the trends\n",
    "\n",
    "# visualize the trends\n",
    "plt.figure(figsize=(20, 10), dpi=100)\n",
    "plt.axhline(y=mean_trend, color='black', linestyle='-', label=f'Mean = {mean_trend:.2f}')\n",
    "plt.axhline(y=mean_trend+std_trend, color='g', linestyle='--', linewidth=1, label=f'1 Std = {std_trend:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend, color='g', linestyle='--', linewidth=1) # No label needed if it's the same category\n",
    "plt.axhline(y=mean_trend+std_trend*2, color='y', linestyle='--', linewidth=3, label=f'2 Std = {std_trend*2:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend*2, color='y', linestyle='--', linewidth=3) # No label needed\n",
    "plt.axhline(y=mean_trend+std_trend*3, color='r', linestyle='--', linewidth=1, label=f'3 Std = {std_trend*3:.2f}')\n",
    "plt.axhline(y=mean_trend-std_trend*3, color='r', linestyle='--', linewidth=1) # No label needed\n",
    "plt.plot(trends['wf_id'], trends['trend'], 'o', markersize=8, label='Wind Farm Trends') # Added a label for the scatter plot\n",
    "plt.xlabel('Wind Farm ID')\n",
    "plt.ylabel('Trend')\n",
    "\n",
    "plt.legend(loc='upper left', ncol=1, frameon=True)\n",
    "\n",
    "plt.title('Wind Farm Trends')\n",
    "plt.show()"
   ],
   "id": "6d84b57ad64d3a1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Calculate Q1, Q3, and IQR for the 'trend' data\n",
    "Q1 = trends['trend'].quantile(0.25)\n",
    "Q3 = trends['trend'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "mean = trends['trend'].mean()\n",
    "\n",
    "print(f\"First Quartile (Q1): {Q1:.2f}\")\n",
    "print(f\"Third Quartile (Q3): {Q3:.2f}\")\n",
    "print(f\"Interquartile Range (IQR): {IQR:.2f}\")\n",
    "print(f\"Mean: {mean:.2f}\")\n",
    "\n",
    "# 2. Create the figure and two subplots side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8), dpi=100, sharey=True) # sharey=True ensures same y-axis scale\n",
    "\n",
    "# Plot data points on the left subplot (axes[0])\n",
    "sns.stripplot(y=trends['trend'], jitter=0.2, color=\"darkblue\", alpha=0.7, ax=axes[0])\n",
    "axes[0].set_ylabel('Trend Value')\n",
    "axes[0].set_title('Individual Data Points')\n",
    "axes[0].set_xticks([]) # Remove x-axis tick labels for the stripplot\n",
    "axes[0].grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot box plot on the right subplot (axes[1])\n",
    "sns.boxplot(y=trends['trend'], color='lightblue', showmeans=True,\n",
    "            meanprops={\"marker\":\"o\", \"markerfacecolor\":\"red\", \"markeredgecolor\":\"black\", \"markersize\":\"8\"},\n",
    "            medianprops={'color': 'orange', 'linewidth': 2},\n",
    "            ax=axes[1])\n",
    "axes[1].set_ylabel('') # No y-label for the second plot since y-axis is shared\n",
    "axes[1].set_title('Box Plot of Trends')\n",
    "axes[1].set_xticks([]) # Remove x-axis tick labels for the box plot\n",
    "axes[1].grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "\n",
    "# Overall title for the figure\n",
    "fig.suptitle('Individual Data Points & Box Plot', fontsize=16)\n",
    "\n",
    "# Create a custom legend for the box plot elements (mean, median) and stripplot points\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='orange', lw=2, label='Median'),\n",
    "    Line2D([0], [0], marker='o', color='w', label='Mean',\n",
    "           markerfacecolor='red', markeredgecolor='black', markersize=8),\n",
    "]\n",
    "# You can place the legend on one of the subplots or outside the figure\n",
    "axes[1].legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1, 1)) # Place outside for clarity\n",
    "axes[0].legend(handles=[Line2D([0], [0], marker='o', color='darkblue', lw=0, label='Individual Data Points',\n",
    "           markerfacecolor='darkblue', markeredgecolor='darkblue', markersize=6)],\n",
    "               loc='upper left', bbox_to_anchor=(0, 1))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
    "plt.show()\n",
    "\n",
    "# 3. (Optional) Programmatically identify and list the outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = trends[(trends['trend'] < lower_bound) | (trends['trend'] > upper_bound)]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(\"\\nOutliers identified based on the 1.5xIQR rule:\")\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"\\nNo outliers found by the 1.5xIQR rule outside the whiskers.\")"
   ],
   "id": "9e706b6c2ebc74a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Choose the type of test: 'one-tailed' or 'two-tailed'.\n",
    "test_type = 'one-tailed'\n",
    "\n",
    "# Use the 'trend' column from your DataFrame\n",
    "trends_data = trends['trend'].values\n",
    "\n",
    "# --- Statistical Analysis ---\n",
    "if len(trends_data) > 0:\n",
    "    # Perform a one-sample t-test\n",
    "    t_statistic, p_value_two_tailed = stats.ttest_1samp(a=trends_data, popmean=0)\n",
    "\n",
    "    # --- Adjust P-value Based on Test Type ---\n",
    "    if test_type == 'one-tailed':\n",
    "        if t_statistic < 0:\n",
    "            p_value = p_value_two_tailed / 2\n",
    "        else:\n",
    "            p_value = 1 - p_value_two_tailed / 2\n",
    "    elif test_type == 'two-tailed':\n",
    "        p_value = p_value_two_tailed\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'test_type'. Please choose either 'one-tailed' or 'two-tailed'.\")\n",
    "\n",
    "    # --- Results and Interpretation ---\n",
    "    mean_trend = np.mean(trends_data)\n",
    "    std_dev = np.std(trends_data, ddof=1)\n",
    "\n",
    "    print(\"\\n--- Statistical Test Results ---\")\n",
    "    print(f\"Analysis based on the trends of {len(trends_data)} wind farms.\")\n",
    "    print(f\"Mean Trend: {mean_trend:.4f}% per year\")\n",
    "    print(f\"Standard Deviation of Trends: {std_dev:.4f}\")\n",
    "    print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "    print(f\"Significance Level (alpha): {alpha}\")\n",
    "    print(f\"Test Type: {test_type}\")\n",
    "    print(f\"Calculated P-value: {p_value:.6f}\")\n",
    "\n",
    "    print(\"\\n--- Conclusion ---\")\n",
    "    if p_value < alpha:\n",
    "        print(f\"The result is statistically significant at the {alpha*100:.0f}% level.\")\n",
    "        if test_type == 'one-tailed':\n",
    "            print(\"We can reject the null hypothesis and conclude that there is a significant degradation trend across the wind farms (the true mean trend is likely less than zero).\")\n",
    "        else:\n",
    "            print(\"We can reject the null hypothesis and conclude that the mean degradation trend is significantly different from zero.\")\n",
    "    else:\n",
    "        print(f\"The result is not statistically significant at the {alpha*100:.0f}% level.\")\n",
    "        print(\"We fail to reject the null hypothesis. There is not enough statistical evidence to conclude that a significant degradation trend exists across the wind farms.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "# Create Q-Q plot to assess normality of the trend data\n",
    "plt.figure(figsize=(12, 8), dpi=100)\n",
    "\n",
    "# Create Q-Q plot using scipy\n",
    "stats.probplot(trends_data, dist=\"norm\", plot=plt)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Q-Q Plot: Trend Data vs Normal Distribution', fontsize=16)\n",
    "plt.xlabel('Theoretical Quantiles (Normal Distribution)', fontsize=12)\n",
    "plt.ylabel('Sample Quantiles (Trend Data)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some statistics to the plot\n",
    "mean_trend = np.mean(trends_data)\n",
    "std_trend = np.std(trends_data, ddof=1)\n",
    "n_samples = len(trends_data)\n",
    "\n",
    "# Add text box with statistics\n",
    "textstr = f'n = {n_samples}\\nMean = {mean_trend:.3f}\\nStd = {std_trend:.3f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "plt.text(0.05, 0.95, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(trends_data)\n",
    "\n",
    "print(\"\\n--- Normality Test Results ---\")\n",
    "print(f\"Shapiro-Wilk Test:\")\n",
    "print(f\"  Statistic: {shapiro_stat:.6f}\")\n",
    "print(f\"  P-value: {shapiro_p:.6f}\")\n",
    "\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"  Result: Data appears to be normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"  Result: Data does not appear to be normally distributed (p ≤ 0.05)\")\n",
    "\n",
    "# Additional normality tests\n",
    "anderson_stat, anderson_critical, anderson_significance = stats.anderson(trends_data, dist='norm')\n",
    "\n",
    "print(f\"\\nAnderson-Darling Test:\")\n",
    "print(f\"  Statistic: {anderson_stat:.6f}\")\n",
    "print(f\"  Critical Values: {anderson_critical}\")\n",
    "print(f\"  Significance Levels: {anderson_significance}%\")\n",
    "\n",
    "# Check if the statistic exceeds critical values\n",
    "for i, (crit_val, sig_level) in enumerate(zip(anderson_critical, anderson_significance)):\n",
    "    if anderson_stat > crit_val:\n",
    "        print(f\"  At {sig_level}% significance level: Reject normality\")\n",
    "    else:\n",
    "        print(f\"  At {sig_level}% significance level: Fail to reject normality\")\n",
    "        break"
   ],
   "id": "499ea46c47c5293a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_sample_size(population_size, margin_of_error, confidence_level=0.95, population_proportion=0.5):\n",
    "    # 1. Calculate the Z-score from the confidence level\n",
    "    z_score = stats.norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "    # 2. Calculate the sample size for an infinite population (Cochran's formula)\n",
    "    n_0 = (z_score**2 * population_proportion * (1 - population_proportion)) / (margin_of_error**2)\n",
    "\n",
    "    # 3. Adjust the sample size for a finite population\n",
    "    n = n_0 / (1 + (n_0 - 1) / population_size)\n",
    "\n",
    "    return math.ceil(n)\n",
    "\n",
    "POPULATION = 297\n",
    "\n",
    "# Scenario 1: 95% confidence, 10% margin of error\n",
    "confidence_95 = 0.95\n",
    "margin_error_10 = 0.10\n",
    "sample_size_10_pct = calculate_sample_size(POPULATION, margin_error_10, confidence_95)\n",
    "print(f\"For a population of {POPULATION}:\")\n",
    "print(f\"Required sample size for 95% confidence and 10% margin of error: {sample_size_10_pct}\")\n",
    "\n",
    "# Scenario 2: 95% confidence, 5% margin of error\n",
    "margin_error_5 = 0.05\n",
    "sample_size_5_pct = calculate_sample_size(POPULATION, margin_error_5, confidence_95)\n",
    "print(f\"Required sample size for 95% confidence and 5% margin of error: {sample_size_5_pct}\")\n",
    "\n",
    "# Scenario 3: 99% confidence, 5% margin of error\n",
    "confidence_99 = 0.99\n",
    "sample_size_99_5_pct = calculate_sample_size(POPULATION, margin_error_5, confidence_99)\n",
    "print(f\"Required sample size for 99% confidence and 5% margin of error: {sample_size_99_5_pct}\")"
   ],
   "id": "2347d23965fe3076",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define the confidence level\n",
    "confidence_level = 0.95\n",
    "\n",
    "# Calculate the necessary parameters from your data\n",
    "degrees_freedom = len(trends_data) - 1\n",
    "sample_mean = np.mean(trends_data)\n",
    "sample_standard_error = stats.sem(trends_data) # sem = Standard Error of the Mean\n",
    "\n",
    "# Calculate the confidence interval using scipy.stats\n",
    "# This function returns the lower and upper bounds of the interval\n",
    "ci = stats.t.interval(confidence_level,\n",
    "                        df=degrees_freedom,\n",
    "                        loc=sample_mean,\n",
    "                        scale=sample_standard_error)\n",
    "\n",
    "# --- Print the results ---\n",
    "print(\"--- 95% Confidence Interval for the Mean Trend ---\")\n",
    "print(f\"Sample Mean: {sample_mean:.4f}% per year\")\n",
    "print(f\"95% Confidence Interval (CI): [{ci[0]:.4f}, {ci[1]:.4f}]\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"We are 95% confident that the true average annual degradation trend for the entire population of Turkish wind farms falls between {ci[0]:.2f}% and {ci[1]:.2f}%.\")\n",
    "if ci[1] < 0:\n",
    "    print(\"Since the entire interval is below zero, this provides strong evidence that a statistically significant degradation is occurring.\")"
   ],
   "id": "58640d393b637944",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# --- Variables to control selective plotting ---\n",
    "plot_kde = True\n",
    "plot_normal_fit = True\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Plot the distribution of trends\n",
    "plt.figure(figsize=(20, 10), dpi=100)\n",
    "\n",
    "# Plot the histogram bars first. We use label='_nolegend_' to hide it from the legend,\n",
    "# as the bars are self-explanatory.\n",
    "ax = sns.histplot(data=trends, x='trend', bins=20, stat=\"density\", label='_nolegend_')\n",
    "\n",
    "# Get the data for fitting\n",
    "data_for_fitting = trends['trend'].dropna()\n",
    "\n",
    "# Plot KDE (Kernel Density Estimate) (Selective)\n",
    "# This plots the smooth line over the histogram and gives it a clear label.\n",
    "if plot_kde:\n",
    "    sns.kdeplot(data=trends, x='trend', color='darkblue', ax=ax, label='KDE')\n",
    "\n",
    "# Plot Normal Distribution Fit (Selective)\n",
    "if plot_normal_fit:\n",
    "    # Fit a normal distribution to the data\n",
    "    mu, std = stats.norm.fit(data_for_fitting)\n",
    "\n",
    "    # Generate x values for the PDF plot\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x_norm = np.linspace(xmin, xmax, 100)\n",
    "\n",
    "    # Calculate PDF of the normal distribution\n",
    "    p_norm = stats.norm.pdf(x_norm, mu, std)\n",
    "\n",
    "    # Plot the normal fit curve with a descriptive label\n",
    "    plt.plot(x_norm, p_norm, 'k', linewidth=2, label=f'Normal Fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "\n",
    "# --- Plot Finalization ---\n",
    "plt.title('Distribution of Capacity Factor Trends', fontsize=16)\n",
    "plt.xlabel('Annual Trend (%/year)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "\n",
    "# Add a vertical line for the mean trend\n",
    "plt.axvline(data_for_fitting.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean Trend ({data_for_fitting.mean():.2f})')\n",
    "# Add a vertical line for the zero (no trend) point\n",
    "plt.axvline(0, color='black', linestyle='-', linewidth=2, label='No Trend')\n",
    "# -----------------\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.show()"
   ],
   "id": "4bedbfeb7ef4f83f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def analyze_trend_by_turbine(\n",
    "    db_path,\n",
    "    trends_csv_path,\n",
    "    single_model_only=False,\n",
    "    plot_type=\"boxplot\",\n",
    "    sort_by=\"trend_desc\",\n",
    "    group_by=\"model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes wind farm capacity factor trends grouped by turbine model or brand.\n",
    "\n",
    "    FINAL CORRECTED LOGIC: This version correctly handles mixed-brand/mixed-model farms\n",
    "    when single_model_only=False by \"exploding\" the data, ensuring all brands/models\n",
    "    are represented in the plot. It also fixes a NameError from the previous version.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The absolute path to the project's SQLite database file (wfd.db).\n",
    "        trends_csv_path (str): The path to the CSV file containing the trend data for each wind farm.\n",
    "        single_model_only (bool): If True, analysis is restricted to single-model/single-brand farms.\n",
    "        plot_type (str): The type of plot to generate, either 'boxplot' or 'barplot'.\n",
    "        sort_by (str): Determines the sorting order of the categories on the x-axis of the plot.\n",
    "        group_by (str): The characteristic to group by, either 'model' or 'brand'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: A DataFrame containing aggregated trend statistics.\n",
    "    \"\"\"\n",
    "    # --- 1. Initial Setup and Data Loading ---\n",
    "    analyzer = DatabaseAnalyzer.WindFarmAnalyzer(db_path)\n",
    "    try:\n",
    "        trends = pd.read_csv(trends_csv_path)\n",
    "        trends = trends.dropna(subset=[\"wf_id\", \"trend\"])\n",
    "        trends[\"wf_id\"] = trends[\"wf_id\"].astype(int)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The trends CSV file was not found at {trends_csv_path}\")\n",
    "        return None\n",
    "\n",
    "    wf_turbines_list = []\n",
    "    for wf_id in trends[\"wf_id\"]:\n",
    "        try:\n",
    "            temp_df = analyzer.get_wf_turbines_data(wf_id)\n",
    "            if temp_df is not None and not temp_df.empty:\n",
    "                temp_df[\"wf_id\"] = wf_id\n",
    "                wf_turbines_list.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve turbine data for wf_id {wf_id}: {e}\")\n",
    "    if not wf_turbines_list:\n",
    "        print(\"No valid turbine data could be fetched.\")\n",
    "        return None\n",
    "    wf_turbines = pd.concat(wf_turbines_list, ignore_index=True)\n",
    "\n",
    "    # --- 2. Data Preparation based on Analysis Type (DEFINITIVE LOGIC) ---\n",
    "    group_col = \"turbine_model\" if group_by == \"model\" else \"turbine_brand\"\n",
    "\n",
    "    # ** BUG FIX: Define display_name here, before it is used **\n",
    "    display_name_map = {'model': 'Model', 'brand': 'Brand'}\n",
    "    display_name = display_name_map.get(group_by, group_by.replace('_', ' ').title())\n",
    "\n",
    "    filter_info_text = \"\"\n",
    "\n",
    "    if single_model_only:\n",
    "        # Strict filtering for 'pure' farms (single model or single brand)\n",
    "        counts = wf_turbines.groupby(\"wf_id\")[group_col].nunique()\n",
    "        single_ids = counts[counts == 1].index.tolist()\n",
    "\n",
    "        filtered_turbines = wf_turbines[wf_turbines[\"wf_id\"].isin(single_ids)]\n",
    "        merged_data = pd.merge(trends, filtered_turbines, on=\"wf_id\", how=\"inner\")\n",
    "\n",
    "        # Create representative data (1 row per farm)\n",
    "        plot_data = merged_data.groupby('wf_id').agg(\n",
    "            trend=('trend', 'first'),\n",
    "            **{group_col: (group_col, 'first')}\n",
    "        ).reset_index()\n",
    "        filter_info_text = f\" (Single-{display_name} Farms Only)\"\n",
    "\n",
    "    else:\n",
    "        # \"All Farms\" analysis, correctly handling mixed farms\n",
    "        groups_per_farm = wf_turbines.groupby('wf_id')[group_col].unique().explode()\n",
    "        plot_data = pd.merge(trends, groups_per_farm, on='wf_id', how=\"inner\")\n",
    "        filter_info_text = \" (All Farms, including mixed)\"\n",
    "        print(\"NOTE: In 'All Farms' mode, a single farm's trend may be counted in multiple groups if it contains multiple brands/models.\")\n",
    "\n",
    "    if plot_data.empty:\n",
    "        print(\"No data available after preparing data for plotting.\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Grouping and Sorting ---\n",
    "    grouped_trends = (\n",
    "        plot_data.groupby(group_col)[\"trend\"]\n",
    "        .agg([\"mean\", \"std\", \"count\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "    if sort_by == \"alphabetical\": grouped_trends = grouped_trends.sort_values(by=group_col)\n",
    "    elif sort_by == \"trend_desc\": grouped_trends = grouped_trends.sort_values(by=\"mean\", ascending=False)\n",
    "    elif sort_by == \"trend_asc\": grouped_trends = grouped_trends.sort_values(by=\"mean\", ascending=True)\n",
    "    elif sort_by == \"count\": grouped_trends = grouped_trends.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    print(f\"\\nGrouped Trends by Turbine {display_name}{filter_info_text}:\\n\", grouped_trends)\n",
    "\n",
    "    # --- 4. Visualization ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    sort_by_map = {\n",
    "        'trend_desc': 'Sorted by Descending Mean Trend', 'trend_asc': 'Sorted by Ascending Mean Trend',\n",
    "        'count': 'Sorted by Farm Count', 'alphabetical': 'Sorted Alphabetically'\n",
    "    }\n",
    "    sort_info_text = sort_by_map.get(sort_by, f\"Sorted by {sort_by}\")\n",
    "\n",
    "    labels_with_counts = [f\"{row[group_col]} (n={row['count']})\" for _, row in grouped_trends.iterrows()]\n",
    "\n",
    "    if plot_type == \"boxplot\":\n",
    "        sns.boxplot(\n",
    "            x=group_col, y=\"trend\", data=plot_data,\n",
    "            order=grouped_trends[group_col], ax=ax, showmeans=True,\n",
    "            meanprops={\"marker\": \"^\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\", \"markersize\": \"8\"},\n",
    "            whiskerprops={'color': 'black', 'linewidth': 1},\n",
    "        )\n",
    "        full_title = f\"Trend Distribution by Turbine {display_name}{filter_info_text}\\n({sort_info_text})\"\n",
    "        ax.set_title(full_title, fontsize=16)\n",
    "\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='C0', alpha=0.4, edgecolor='C0', label='Interquartile Range (IQR: 25%-75%)'),\n",
    "            Line2D([0], [0], color='black', lw=1, label='Whiskers (Range within 1.5*IQR)'),\n",
    "            Line2D([0], [0], color=ax.lines[4].get_color(), lw=2, label='Median'),\n",
    "            Line2D([0], [0], marker='^', color='w', label='Mean', markerfacecolor='white', markeredgecolor='black', markersize=10)\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='best', frameon=True)\n",
    "\n",
    "    elif plot_type == \"barplot\":\n",
    "        bars = ax.bar(grouped_trends[group_col], grouped_trends[\"mean\"], yerr=grouped_trends[\"std\"], capsize=5)\n",
    "        full_title = f\"Mean Trend by Turbine {display_name}{filter_info_text}\\n({sort_info_text})\"\n",
    "        ax.set_title(full_title, fontsize=16)\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2.0, yval, f\"{yval:.2f}\", va=\"bottom\" if yval >= 0 else \"top\", ha=\"center\", fontsize=9)\n",
    "\n",
    "    ax.set_xticks(range(len(labels_with_counts)))\n",
    "    ax.set_xticklabels(labels_with_counts, rotation=90, ha='right')\n",
    "    ax.set_xlabel(f\"Turbine {display_name}\", fontsize=12)\n",
    "    ax.set_ylabel(\"Trend (%/year)\", fontsize=12)\n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    tick_min = np.floor(ymin / 0.25) * 0.25\n",
    "    tick_max = np.ceil(ymax / 0.25) * 0.25\n",
    "    ax.set_yticks(np.arange(tick_min, tick_max + 0.25, 0.25))\n",
    "\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"figures_for_thesis/trends_for_{}_by_turbine_{}.png\".format(display_name, group_by), dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "    return grouped_trends"
   ],
   "id": "e2468eb0f91fa0e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example Usage\n",
    "db_path = \"/home/wheatley/WFD/wfd.db\"  # Replace with your database path\n",
    "trends_csv_path = \"trends.csv\"\n",
    "\n",
    "# Run for all farms\n",
    "all_farms_results = analyze_trend_by_turbine(db_path, trends_csv_path, single_model_only=False, sort_by=\"trend_desc\") # \"alphabetical\" \"trend_asc\" \"trend_desc\" \"count\""
   ],
   "id": "95f52552f2b906f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "single_model_results = analyze_trend_by_turbine(db_path, trends_csv_path, single_model_only=True, sort_by=\"trend_desc\")",
   "id": "57711e3168e3eb2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_results = analyze_trend_by_turbine(db_path, trends_csv_path, single_model_only=True, group_by='brand', sort_by='trend_desc')",
   "id": "683f93eef27e6b8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def analyze_trend_by_elevation(db_path, trends_csv_path):\n",
    "    \"\"\"\n",
    "    Analyzes and plots the relationship between wind farm elevation and capacity factor trends.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): Path to the SQLite database.\n",
    "        trends_csv_path (str): Path to the CSV file containing trend data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing the correlation matrix between trend and elevation.\n",
    "    \"\"\"\n",
    "    # --- Initial Setup ---\n",
    "    analyzer = DatabaseAnalyzer.WindFarmAnalyzer(db_path)\n",
    "    trends = pd.read_csv(trends_csv_path)\n",
    "    trends = trends.dropna(subset=['trend'])\n",
    "    trends['wf_id'] = trends['wf_id'].astype(int)\n",
    "\n",
    "    # --- Fetch and Process Elevation Data ---\n",
    "    elevations = []\n",
    "    for wf_id in trends['wf_id']:\n",
    "        try:\n",
    "            el = analyzer.find_elevations(wf_id)\n",
    "            if el:\n",
    "                # Calculate average elevation in meters and then convert to kilometers\n",
    "                avg_elevation_meters = sum(x[3] for x in el) / len(el)\n",
    "                elevations.append(avg_elevation_meters / 1000.0) # Convert to km\n",
    "            else:\n",
    "                elevations.append(None)\n",
    "        except (KeyError, IndexError):\n",
    "            # Handle cases where wf_id is not found or elevation data is malformed\n",
    "            elevations.append(None)\n",
    "\n",
    "    trends['elevation'] = elevations\n",
    "\n",
    "    # Drop rows where elevation could not be determined to ensure clean calculations\n",
    "    trends.dropna(subset=['elevation'], inplace=True)\n",
    "\n",
    "    # --- Plotting ---\n",
    "    # UPDATED: Set figure size to match other plots\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create scatter plot\n",
    "    ax.scatter(trends['elevation'], trends['trend'], label='WPP Data Points', alpha=0.7, zorder=5)\n",
    "\n",
    "    # Calculate and plot the trendline\n",
    "    if not trends.empty:\n",
    "        # NOTE: r_value is no longer needed for the label\n",
    "        slope, intercept, _, _, _ = stats.linregress(trends['elevation'], trends['trend'])\n",
    "\n",
    "        # UPDATED: Simplified the legend label to remove R-squared\n",
    "        formula_label = f'Trendline (y = {slope:.4f}x + {intercept:.2f})'\n",
    "\n",
    "        ax.plot(trends['elevation'], slope * trends['elevation'] + intercept,\n",
    "                color='red', label=formula_label, zorder=10)\n",
    "\n",
    "    # Set a clean, consistent title\n",
    "    ax.set_title('Capacity Factor Trend vs. Mean Site Elevation', fontsize=16)\n",
    "\n",
    "    # Set consistent labels and fonts\n",
    "    ax.set_xlabel('Mean Site Elevation (km)', fontsize=12)\n",
    "    ax.set_ylabel('Trend (%/year)', fontsize=12)\n",
    "\n",
    "    # Set y-axis ticks to increments of 0.25\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    tick_min = np.floor(ymin / 0.25) * 0.25\n",
    "    tick_max = np.ceil(ymax / 0.25) * 0.25\n",
    "    ax.set_yticks(np.arange(tick_min, tick_max + 0.25, 0.25))\n",
    "\n",
    "    # Add grid and reference line for consistency\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Display legend\n",
    "    ax.legend(frameon=True, loc='best')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(\"figures_for_thesis/Capacity trend by Elevation (by Trends)')\")\n",
    "    plt.show()\n",
    "\n",
    "    # Return the correlation matrix for verification\n",
    "    correlation_matrix = trends[['trend', 'elevation']].corr()\n",
    "    print(\"Correlation Matrix:\")\n",
    "    print(correlation_matrix)\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "# --- How to use the new function ---\n",
    "# Define your database and trends file paths\n",
    "db_path = os.path.abspath('/home/wheatley/WFD/wfd.db')\n",
    "trends_csv_path = 'trends.csv'\n",
    "\n",
    "# Call the function to generate the plot and get the correlation\n",
    "correlation_result = analyze_trend_by_elevation(db_path, trends_csv_path)"
   ],
   "id": "c8555c13fd033cb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "def analyze_turbine_trends_by_year(\n",
    "    db_path,\n",
    "    trends_csv_path,\n",
    "    single_model_only=False,\n",
    "    sort_by=\"year_asc\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes wind farm capacity factor trends grouped by installation year.\n",
    "    (Args and initial logic remain the same)\n",
    "    \"\"\"\n",
    "    # --- 1. Initial Setup and Data Loading ---\n",
    "    analyzer = DatabaseAnalyzer.WindFarmAnalyzer(db_path)\n",
    "    try:\n",
    "        trends = pd.read_csv(trends_csv_path)\n",
    "        trends = trends.dropna(subset=[\"wf_id\", \"trend\"])\n",
    "        trends[\"wf_id\"] = trends[\"wf_id\"].astype(int)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The trends CSV file was not found at {trends_csv_path}\")\n",
    "        return None\n",
    "\n",
    "    wf_turbines_list = []\n",
    "    for wf_id in trends[\"wf_id\"]:\n",
    "        try:\n",
    "            temp_df = analyzer.get_wf_turbines_data(wf_id)\n",
    "            if temp_df is not None and not temp_df.empty:\n",
    "                temp_df[\"start_date_of_operation\"] = pd.to_datetime(\n",
    "                    temp_df[\"start_date_of_operation\"], errors=\"coerce\"\n",
    "                )\n",
    "                temp_df.dropna(subset=[\"start_date_of_operation\"], inplace=True)\n",
    "                if not temp_df.empty:\n",
    "                    temp_df[\"wf_id\"] = wf_id\n",
    "                    wf_turbines_list.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not retrieve or process turbine data for wf_id {wf_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not wf_turbines_list:\n",
    "        print(\"No valid turbine data could be fetched from the database.\")\n",
    "        return None\n",
    "    wf_turbines = pd.concat(wf_turbines_list, ignore_index=True)\n",
    "\n",
    "    # --- 2. Conditional Filtering ---\n",
    "    if single_model_only:\n",
    "        # ... (This logic is unchanged) ...\n",
    "        def check_homogeneity(farm_data):\n",
    "            is_single_model = farm_data[\"turbine_model\"].nunique() == 1\n",
    "            is_single_year = farm_data[\"start_date_of_operation\"].dt.year.nunique() == 1\n",
    "            return is_single_model and is_single_year\n",
    "        homogeneous_wf_ids = [\n",
    "            wf_id for wf_id, group_df in wf_turbines.groupby(\"wf_id\") if check_homogeneity(group_df)\n",
    "        ]\n",
    "        if not homogeneous_wf_ids:\n",
    "            print(\"No wind farms met the homogeneity criteria.\")\n",
    "            return None\n",
    "        wf_turbines = wf_turbines[wf_turbines[\"wf_id\"].isin(homogeneous_wf_ids)]\n",
    "\n",
    "    # --- 3. Merging and Preparing Representative Data ---\n",
    "    # ... (This logic is unchanged) ...\n",
    "    merged_data = pd.merge(trends, wf_turbines, on=\"wf_id\", how=\"inner\")\n",
    "    if merged_data.empty:\n",
    "        print(\"No data available after merging.\")\n",
    "        return None\n",
    "    representative_data = merged_data.groupby(\"wf_id\").agg(\n",
    "        trend=(\"trend\", \"first\"),\n",
    "        installation_year=(\"start_date_of_operation\", lambda x: x.dt.year.min()),\n",
    "    ).reset_index()\n",
    "    if representative_data.empty:\n",
    "        print(\"Data became empty after creating the representative dataset.\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Grouping and Sorting ---\n",
    "    # ... (This logic is unchanged) ...\n",
    "    grouped_trends = (\n",
    "        representative_data.groupby(\"installation_year\")[\"trend\"]\n",
    "        .agg([\"mean\", \"std\", \"count\"]).reset_index()\n",
    "    )\n",
    "    if sort_by == \"year_asc\":\n",
    "        grouped_trends = grouped_trends.sort_values(by=\"installation_year\", ascending=True)\n",
    "    elif sort_by == \"trend_desc\":\n",
    "        grouped_trends = grouped_trends.sort_values(by=\"mean\", ascending=False)\n",
    "    elif sort_by == \"trend_asc\":\n",
    "        grouped_trends = grouped_trends.sort_values(by=\"mean\", ascending=True)\n",
    "    elif sort_by == \"count\":\n",
    "        grouped_trends = grouped_trends.sort_values(by=\"count\", ascending=False)\n",
    "    else:\n",
    "        grouped_trends = grouped_trends.sort_values(by=\"installation_year\", ascending=True)\n",
    "\n",
    "    print(\"\\nGrouped Trends by Installation Year:\\n\", grouped_trends)\n",
    "\n",
    "    # --- 5. Visualization (UPDATED) ---\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create descriptive text for sorting and filtering\n",
    "    sort_by_map = {\n",
    "        'year_asc': 'Sorted by Year',\n",
    "        'trend_desc': 'Sorted by Descending Mean Trend',\n",
    "        'trend_asc': 'Sorted by Ascending Mean Trend',\n",
    "        'count': 'Sorted by Data Count'\n",
    "    }\n",
    "    sort_info_text = sort_by_map.get(sort_by, f\"Sorted by {sort_by}\")\n",
    "    filter_info_text = \" (Homogeneous Farms Only)\" if single_model_only else \" (All Farms)\"\n",
    "\n",
    "    # Create labels with data counts for the x-axis\n",
    "    labels_with_counts = [\n",
    "        f\"{int(row['installation_year'])} (n={int(row['count'])})\" for _, row in grouped_trends.iterrows()\n",
    "    ]\n",
    "\n",
    "    # Create the boxplot with consistent styling\n",
    "    sns.boxplot(\n",
    "        x=\"installation_year\", y=\"trend\", data=representative_data,\n",
    "        order=grouped_trends[\"installation_year\"], ax=ax, showmeans=True,\n",
    "        meanprops={\"marker\": \"^\", \"markerfacecolor\": \"white\", \"markeredgecolor\": \"black\", \"markersize\": \"8\"},\n",
    "        whiskerprops={'color': 'black', 'linewidth': 1},\n",
    "    )\n",
    "\n",
    "    # Set the full, descriptive title\n",
    "    full_title = f\"Distribution of CF Trends by Installation Year{filter_info_text}\\n({sort_info_text})\"\n",
    "    ax.set_title(full_title, fontsize=16)\n",
    "\n",
    "    # Add the detailed legend explaining the boxplot components\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='C0', alpha=0.4, edgecolor='C0', label='Interquartile Range (IQR: 25%-75%)'),\n",
    "        Line2D([0], [0], color='black', lw=1, label='Whiskers (Range within 1.5*IQR)'),\n",
    "        Line2D([0], [0], color=ax.lines[4].get_color(), lw=2, label='Median'),\n",
    "        Line2D([0], [0], marker='^', color='w', label='Mean',\n",
    "               markerfacecolor='white', markeredgecolor='black', markersize=10)\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='best', frameon=True)\n",
    "\n",
    "    # Apply consistent axis labels and ticks\n",
    "    ax.set_xticks(range(len(labels_with_counts)))\n",
    "    ax.set_xticklabels(labels_with_counts, rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Installation Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Capacity Factor Trend (%/year)\", fontsize=12)\n",
    "\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    tick_min = np.floor(ymin / 0.25) * 0.25\n",
    "    tick_max = np.ceil(ymax / 0.25) * 0.25\n",
    "    ax.set_yticks(np.arange(tick_min, tick_max + 0.25, 0.25))\n",
    "\n",
    "    ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"figures_for_thesis/Capacity Trend by Installation Year (by Trends) (single modal={single_model_only})\".format(single_model_only=single_model_only),dpi=100)\n",
    "    plt.show()\n",
    "\n",
    "    return grouped_trends\n",
    "\n",
    "\n",
    "db_path = os.path.abspath('/home/wheatley/WFD/wfd.db')\n",
    "trends_csv_path = 'trends.csv'\n",
    "\n",
    "results_all_farms = analyze_turbine_trends_by_year(\n",
    "    db_path,\n",
    "    trends_csv_path,\n",
    "    single_model_only=True,\n",
    "    sort_by=\"year_asc\",\n",
    ")"
   ],
   "id": "b99b31ac85f090bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_all_farms = analyze_turbine_trends_by_year(\n",
    "    db_path,\n",
    "    trends_csv_path,\n",
    "    single_model_only=False,\n",
    "    sort_by=\"year_asc\",\n",
    ")"
   ],
   "id": "8c3ffa8fdc23cd61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from shapely import wkb # For Well-Known Binary\n",
    "import numpy as np\n",
    "\n",
    "def analyze_farm_size_layout_correlations(db_path, trends_csv_path):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes correlations between CF trends and\n",
    "    farm size/layout characteristics (number of turbines, total capacity, capacity density).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Data Loading and SpatiaLite Connection (Unchanged) ---\n",
    "        trends_df = pd.read_csv(trends_csv_path)\n",
    "        trends_df = trends_df.dropna(subset=['wf_id', 'trend'])\n",
    "        trends_df['wf_id'] = trends_df['wf_id'].astype(int)\n",
    "\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        conn.enable_load_extension(True)\n",
    "        try:\n",
    "            conn.execute(\"SELECT load_extension('mod_spatialite')\")\n",
    "        except sqlite3.OperationalError:\n",
    "            try:\n",
    "                conn.execute(\"SELECT load_extension('/usr/lib/x86_64-linux-gnu/mod_spatialite.so')\")\n",
    "            except sqlite3.OperationalError as e:\n",
    "                print(f\"Warning: Could not load SpatiaLite extension. Spatial functions might fail. Error: {e}\")\n",
    "\n",
    "        # --- Data Querying and Processing (Unchanged) ---\n",
    "        query_turbines = \"SELECT wf_id, SUM(turbine_number) as num_turbines FROM wf_turbines GROUP BY wf_id\"\n",
    "        num_turbines_df = pd.read_sql_query(query_turbines, conn)\n",
    "        if not num_turbines_df.empty:\n",
    "            num_turbines_df['wf_id'] = pd.to_numeric(num_turbines_df['wf_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "        query_wf_info = \"SELECT wf_id, installed_power_electrical, ST_AsBinary(farm_boundary_geom) AS farm_boundary_wkb FROM wf\"\n",
    "        wf_info_df = pd.read_sql_query(query_wf_info, conn)\n",
    "        if not wf_info_df.empty:\n",
    "            wf_info_df['wf_id'] = pd.to_numeric(wf_info_df['wf_id'], errors='coerce').astype('Int64')\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        merged_df = pd.merge(trends_df, num_turbines_df, on='wf_id', how='inner')\n",
    "        merged_df = pd.merge(merged_df, wf_info_df, on='wf_id', how='inner')\n",
    "\n",
    "        if merged_df.empty:\n",
    "            print(\"Merging resulted in an empty DataFrame.\")\n",
    "            return None\n",
    "\n",
    "        def calculate_area_from_standard_wkb(wkb_input):\n",
    "            if pd.isna(wkb_input) or wkb_input is None: return np.nan\n",
    "            bytes_to_parse = wkb_input if isinstance(wkb_input, bytes) else bytes.fromhex(wkb_input)\n",
    "            try:\n",
    "                return wkb.loads(bytes_to_parse).area\n",
    "            except Exception:\n",
    "                return np.nan\n",
    "\n",
    "        merged_df['farm_area_sq_m'] = merged_df['farm_boundary_wkb'].apply(calculate_area_from_standard_wkb)\n",
    "        merged_df['farm_area_sq_km'] = merged_df['farm_area_sq_m'] / 1_000_000\n",
    "        valid_area_mask = merged_df['farm_area_sq_km'].notna() & (merged_df['farm_area_sq_km'] > 1e-9)\n",
    "        merged_df.loc[valid_area_mask, 'capacity_density_MWe_per_sq_km'] = merged_df.loc[valid_area_mask, 'installed_power_electrical'] / merged_df.loc[valid_area_mask, 'farm_area_sq_km']\n",
    "\n",
    "        # --- Create Scatter Plots (UPDATED) ---\n",
    "        metrics_to_plot = {\n",
    "            'num_turbines': 'Number of Turbines',\n",
    "            'installed_power_electrical': 'Total Installed Capacity (MWe)',\n",
    "            'capacity_density_MWe_per_sq_km': 'Capacity Density (MWe/km²)'\n",
    "        }\n",
    "\n",
    "        for col, name in metrics_to_plot.items():\n",
    "            plot_df = merged_df.dropna(subset=['trend', col])\n",
    "            if plot_df.empty or len(plot_df) < 2:\n",
    "                print(f\"Skipping plot for {name} due to insufficient valid data.\")\n",
    "                continue\n",
    "\n",
    "            # Create a separate, styled figure for each metric\n",
    "            fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "            # 1. Plot the scatter points\n",
    "            ax.scatter(x=plot_df[col], y=plot_df['trend'], label='WPP Data Points', alpha=0.7)\n",
    "\n",
    "            # 2. Calculate and plot the regression line\n",
    "            slope, intercept, _, _, _ = stats.linregress(plot_df[col], plot_df['trend'])\n",
    "            formula_label = f'Trendline (y = {slope:.4f}x + {intercept:.2f})'\n",
    "\n",
    "            # Create a smooth line for plotting\n",
    "            x_vals = np.linspace(plot_df[col].min(), plot_df[col].max(), 100)\n",
    "            y_vals = slope * x_vals + intercept\n",
    "            ax.plot(x_vals, y_vals, color='red', label=formula_label)\n",
    "\n",
    "            # 3. Apply all consistent styling elements\n",
    "            ax.set_title(f'Capacity Factor Trend vs. {name}', fontsize=16)\n",
    "            ax.set_xlabel(name, fontsize=12)\n",
    "            ax.set_ylabel('Trend (%/year)', fontsize=12)\n",
    "\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            tick_min = np.floor(ymin / 0.25) * 0.25\n",
    "            tick_max = np.ceil(ymax / 0.25) * 0.25\n",
    "            ax.set_yticks(np.arange(tick_min, tick_max + 0.25, 0.25))\n",
    "\n",
    "            ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
    "            ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "            ax.legend(frameon=True, loc='best')\n",
    "            fig.tight_layout()\n",
    "            plt.savefig(\"figures_for_thesis/Capacity trend by {name} (by Trends)\".format(name=col), dpi=100)\n",
    "            plt.show()\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in analyze_farm_size_layout_correlations: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# --- Example Usage ---\n",
    "db_path = \"/home/wheatley/WFD/wfd.db\"\n",
    "trends_csv_path = \"trends.csv\"\n",
    "farm_size_layout_df = analyze_farm_size_layout_correlations(db_path, trends_csv_path)"
   ],
   "id": "669fd1d034b31e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries for analysis and display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from IPython.display import display, Markdown\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "\n",
    "# ==============================================================================\n",
    "# DATA PREPARATION FOR ALL ANALYSES\n",
    "# ==============================================================================\n",
    "display(Markdown(\"## Final Summary Tables & Statistical Tests\"))\n",
    "display(Markdown(\"This section consolidates the key findings. Each analysis is performed on the most appropriate subset of the data to ensure results are scientifically robust and clear.\"))\n",
    "\n",
    "# --- 1. Data Consolidation ---\n",
    "print(\"--- Consolidating all data for summary tables... ---\")\n",
    "db_path = \"/home/wheatley/WFD/wfd.db\"\n",
    "trends_csv_path = \"trends.csv\"\n",
    "\n",
    "# Load trends data\n",
    "trends_df = pd.read_csv(trends_csv_path)\n",
    "trends_df = trends_df.dropna(subset=['wf_id', 'trend'])\n",
    "trends_df['wf_id'] = trends_df['wf_id'].astype(int)\n",
    "\n",
    "# ... (Database connection and data loading logic remains the same) ...\n",
    "analyzer = DatabaseAnalyzer.WindFarmAnalyzer(db_path)\n",
    "conn = sqlite3.connect(db_path)\n",
    "conn.enable_load_extension(True)\n",
    "try:\n",
    "    conn.execute(\"SELECT load_extension('mod_spatialite')\")\n",
    "except sqlite3.OperationalError:\n",
    "    try: conn.execute(\"SELECT load_extension('/usr/lib/x86_64-linux-gnu/mod_spatialite.so')\")\n",
    "    except Exception as e: print(f\"Warning: SpatiaLite load failed: {e}\")\n",
    "\n",
    "num_turbines_df = pd.read_sql_query(\"SELECT wf_id, SUM(turbine_number) as num_turbines FROM wf_turbines GROUP BY wf_id\", conn)\n",
    "wf_turbines_details_df = pd.read_sql_query(\"SELECT wf_id, turbine_brand, turbine_model, start_date_of_operation FROM wf_turbines\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Merge all data sources into a detailed dataframe\n",
    "summary_df_full_details = pd.merge(trends_df, num_turbines_df, on='wf_id', how='left')\n",
    "summary_df_full_details = pd.merge(summary_df_full_details, wf_turbines_details_df, on='wf_id', how='left')\n",
    "summary_df_full_details['start_date_of_operation'] = pd.to_datetime(summary_df_full_details['start_date_of_operation'], errors='coerce')\n",
    "\n",
    "# --- 2. Create Purpose-Built, Filtered Datasets ---\n",
    "print(\"--- Preparing filtered datasets for specific analyses... ---\")\n",
    "\n",
    "# Base representative dataset (one row per farm)\n",
    "representative_df_base = summary_df_full_details.groupby('wf_id').agg(\n",
    "    trend=('trend', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Filter 1: Single-Brand Farms\n",
    "single_brand_wf_ids = [wf_id for wf_id, group_df in summary_df_full_details.groupby(\"wf_id\") if group_df[\"turbine_brand\"].nunique() == 1]\n",
    "representative_df_single_brand = summary_df_full_details[summary_df_full_details['wf_id'].isin(single_brand_wf_ids)].groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'),\n",
    "    turbine_brand=('turbine_brand', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Filter 2: Single-Model Farms\n",
    "single_model_wf_ids = [wf_id for wf_id, group_df in summary_df_full_details.groupby(\"wf_id\") if group_df[\"turbine_model\"].nunique() == 1]\n",
    "representative_df_single_model = summary_df_full_details[summary_df_full_details['wf_id'].isin(single_model_wf_ids)].groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'),\n",
    "    turbine_model=('turbine_model', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Filter 3: Homogeneous Farms\n",
    "def check_homogeneity(farm_data):\n",
    "    return farm_data[\"turbine_model\"].nunique() == 1 and farm_data[\"start_date_of_operation\"].dt.year.nunique() == 1\n",
    "homogeneous_wf_ids = [wf_id for wf_id, group_df in summary_df_full_details.groupby(\"wf_id\") if check_homogeneity(group_df)]\n",
    "representative_df_homogeneous = summary_df_full_details[summary_df_full_details['wf_id'].isin(homogeneous_wf_ids)].groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'),\n",
    "    # *** THIS IS THE FIX: Directly aggregate the year component ***\n",
    "    installation_year=('start_date_of_operation', lambda x: x.dt.year.min())\n",
    ").reset_index()\n",
    "\n",
    "print(f\"\\nTotal unique farms in dataset: {len(representative_df_base)}\")\n",
    "print(f\"Number of Single-Brand farms: {len(representative_df_single_brand)}\")\n",
    "print(f\"Number of Single-Model farms: {len(representative_df_single_model)}\")\n",
    "print(f\"Number of Homogeneous farms: {len(representative_df_homogeneous)}\")\n",
    "print(\"--- Generating final tables. ---\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SUMMARY TABLES\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Table 1: Trend Distribution by Turbine Brand (Single-Brand Farms) ---\n",
    "display(Markdown(\"### Summary Table 1: Trend Distribution by Turbine Brand (Single-Brand Farms)\"))\n",
    "brand_summary_table = representative_df_single_brand.groupby('turbine_brand')['trend'].agg(['count', 'mean', 'median']).reset_index()\n",
    "brand_summary_table = brand_summary_table.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "display(brand_summary_table)\n",
    "\n",
    "# --- Table 2: Trend Distribution by Turbine Model (Single-Model Farms) ---\n",
    "display(Markdown(\"\\n### Summary Table 2: Trend Distribution by Turbine Model (Single-Model Farms)\"))\n",
    "model_summary_table = representative_df_single_model.groupby('turbine_model')['trend'].agg(['count', 'mean', 'median']).reset_index()\n",
    "model_summary_table = model_summary_table.sort_values(by='count', ascending=False).reset_index(drop=True)\n",
    "display(model_summary_table)\n",
    "\n",
    "# --- Table 3: Trend Distribution by Installation Year (Homogeneous Farms) ---\n",
    "display(Markdown(\"\\n### Summary Table 3: Trend Distribution by Installation Year (Homogeneous Farms)\"))\n",
    "year_summary_table = representative_df_homogeneous.groupby('installation_year')['trend'].agg(['count', 'mean', 'median']).reset_index()\n",
    "year_summary_table['installation_year'] = year_summary_table['installation_year'].astype(int)\n",
    "year_summary_table = year_summary_table.sort_values(by='installation_year', ascending=True).reset_index(drop=True)\n",
    "display(year_summary_table)"
   ],
   "id": "b9c570558f1e21b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries for analysis and display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from IPython.display import display, Markdown\n",
    "from src.DatabaseGetInfo import DatabaseAnalyzer\n",
    "\n",
    "# ==============================================================================\n",
    "# DATA PREPARATION FOR ALL STATISTICAL TESTS\n",
    "# ==============================================================================\n",
    "display(Markdown(\"## Final Statistical Significance Tests\"))\n",
    "display(Markdown(\"This section performs quantitative statistical tests to validate the visual findings. Each test is run on the most appropriate dataset to ensure the conclusions are scientifically sound.\"))\n",
    "\n",
    "# --- 1. Data Consolidation ---\n",
    "print(\"--- Consolidating all data for tests... ---\")\n",
    "db_path = \"/home/wheatley/WFD/wfd.db\"\n",
    "trends_csv_path = \"trends.csv\"\n",
    "\n",
    "# Load trends data\n",
    "trends_df = pd.read_csv(trends_csv_path)\n",
    "trends_df = trends_df.dropna(subset=['wf_id', 'trend'])\n",
    "trends_df['wf_id'] = trends_df['wf_id'].astype(int)\n",
    "\n",
    "# ... (Database connection and data loading logic) ...\n",
    "analyzer = DatabaseAnalyzer.WindFarmAnalyzer(db_path)\n",
    "conn = sqlite3.connect(db_path)\n",
    "conn.enable_load_extension(True)\n",
    "try:\n",
    "    conn.execute(\"SELECT load_extension('mod_spatialite')\")\n",
    "except sqlite3.OperationalError:\n",
    "    try: conn.execute(\"SELECT load_extension('/usr/lib/x86_64-linux-gnu/mod_spatialite.so')\")\n",
    "    except Exception as e: print(f\"Warning: SpatiaLite load failed: {e}\")\n",
    "\n",
    "elevations = []\n",
    "for wf_id in trends_df['wf_id']:\n",
    "    try:\n",
    "        el = analyzer.find_elevations(wf_id)\n",
    "        if el: elevations.append(sum(x[3] for x in el) / len(el) / 1000.0) # in km\n",
    "        else: elevations.append(np.nan)\n",
    "    except (KeyError, IndexError): elevations.append(np.nan)\n",
    "trends_df['elevation'] = elevations\n",
    "\n",
    "num_turbines_df = pd.read_sql_query(\"SELECT wf_id, SUM(turbine_number) as num_turbines FROM wf_turbines GROUP BY wf_id\", conn)\n",
    "wf_info_df = pd.read_sql_query(\"SELECT wf_id, installed_power_electrical, ST_AsBinary(farm_boundary_geom) AS farm_boundary_wkb FROM wf\", conn)\n",
    "wf_turbines_details_df = pd.read_sql_query(\"SELECT wf_id, turbine_brand, turbine_model, start_date_of_operation FROM wf_turbines\", conn)\n",
    "conn.close()\n",
    "\n",
    "# Merge all data sources into a detailed dataframe\n",
    "summary_df_full_details = pd.merge(trends_df, num_turbines_df, on='wf_id', how='left')\n",
    "summary_df_full_details = pd.merge(summary_df_full_details, wf_info_df, on='wf_id', how='left')\n",
    "summary_df_full_details = pd.merge(summary_df_full_details, wf_turbines_details_df, on='wf_id', how='left')\n",
    "summary_df_full_details['start_date_of_operation'] = pd.to_datetime(summary_df_full_details['start_date_of_operation'], errors='coerce')\n",
    "\n",
    "def calculate_area_from_standard_wkb(wkb_input):\n",
    "    if pd.isna(wkb_input): return np.nan\n",
    "    try: return wkb.loads(wkb_input if isinstance(wkb_input, bytes) else bytes.fromhex(wkb_input)).area\n",
    "    except: return np.nan\n",
    "summary_df_full_details['farm_area_sq_km'] = summary_df_full_details['farm_boundary_wkb'].apply(calculate_area_from_standard_wkb) / 1_000_000\n",
    "valid_area_mask = summary_df_full_details['farm_area_sq_km'].notna() & (summary_df_full_details['farm_area_sq_km'] > 1e-9)\n",
    "summary_df_full_details.loc[valid_area_mask, 'capacity_density_MWe_per_sq_km'] = summary_df_full_details.loc[valid_area_mask, 'installed_power_electrical'] / summary_df_full_details.loc[valid_area_mask, 'farm_area_sq_km']\n",
    "\n",
    "\n",
    "# --- 2. Create Purpose-Built, Representative Datasets ---\n",
    "print(\"--- Preparing representative datasets for specific analyses... ---\")\n",
    "\n",
    "# Dataset for Continuous Variable analysis (All Farms)\n",
    "representative_df_all_farms = summary_df_full_details.groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'),\n",
    "    elevation=('elevation', 'first'),\n",
    "    num_turbines=('num_turbines', 'first'),\n",
    "    installed_power_electrical=('installed_power_electrical', 'first'),\n",
    "    capacity_density_MWe_per_sq_km=('capacity_density_MWe_per_sq_km', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Dataset for Brand analysis (Single-Brand Farms)\n",
    "single_brand_wf_ids = [wf_id for wf_id, g in summary_df_full_details.groupby(\"wf_id\") if g[\"turbine_brand\"].nunique() == 1]\n",
    "representative_df_single_brand = summary_df_full_details[summary_df_full_details['wf_id'].isin(single_brand_wf_ids)].groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'), turbine_brand=('turbine_brand', 'first')\n",
    ").reset_index()\n",
    "\n",
    "# Dataset for Year analysis (Homogeneous Farms)\n",
    "def check_homogeneity(farm_data):\n",
    "    return farm_data[\"turbine_model\"].nunique() == 1 and farm_data[\"start_date_of_operation\"].dt.year.nunique() == 1\n",
    "homogeneous_wf_ids = [wf_id for wf_id, g in summary_df_full_details.groupby(\"wf_id\") if check_homogeneity(g)]\n",
    "representative_df_homogeneous = summary_df_full_details[summary_df_full_details['wf_id'].isin(homogeneous_wf_ids)].groupby('wf_id').agg(\n",
    "    trend=('trend', 'first'), installation_year=('start_date_of_operation', lambda x: x.dt.year.min())\n",
    ").reset_index()\n",
    "\n",
    "print(f\"\\nTotal unique farms in dataset: {len(representative_df_all_farms)}\")\n",
    "print(f\"Number of Single-Brand farms: {len(representative_df_single_brand)}\")\n",
    "print(f\"Number of Homogeneous farms: {len(representative_df_homogeneous)}\")\n",
    "print(\"--- Running final statistical tests on appropriate datasets. ---\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# STATISTICAL SIGNIFICANCE TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Test A: Differences Among Turbine Brands (on Single-Brand Farms) ---\n",
    "display(Markdown(\"### Test A: Significance of Differences Among Turbine Brands (Single-Brand Farms)\"))\n",
    "df_test_brand = representative_df_single_brand\n",
    "brand_groups = [group['trend'].values for name, group in df_test_brand.groupby('turbine_brand')]\n",
    "f_statistic, anova_p_value = stats.f_oneway(*brand_groups)\n",
    "display(Markdown(f\"**ANOVA Test Result:** F-statistic = {f_statistic:.4f}, P-value = {anova_p_value:.4f}\"))\n",
    "if anova_p_value > 0.05:\n",
    "    print(\"Conclusion: The high p-value indicates no statistically significant evidence of a difference in mean trends among the brands.\")\n",
    "\n",
    "# --- Test B: Differences Among Installation Years (on Homogeneous Farms) ---\n",
    "display(Markdown(\"\\n### Test B: Significance of Differences Among Installation Years (Homogeneous Farms)\"))\n",
    "df_test_year = representative_df_homogeneous.dropna(subset=['installation_year'])\n",
    "year_groups = [group['trend'].values for name, group in df_test_year.groupby('installation_year')]\n",
    "if len(year_groups) > 1:\n",
    "    f_statistic_yr, anova_p_value_yr = stats.f_oneway(*year_groups)\n",
    "    display(Markdown(f\"**ANOVA Test Result:** F-statistic = {f_statistic_yr:.4f}, P-value = {anova_p_value_yr:.4f}\"))\n",
    "    if anova_p_value_yr > 0.05:\n",
    "        print(\"Conclusion: The high p-value indicates no statistical evidence of a difference in mean trends among the installation years.\")\n",
    "else:\n",
    "    print(\"Not enough year diversity in the homogeneous dataset to perform ANOVA.\")\n",
    "\n",
    "# --- Test C: Differences for Binned Continuous Variables (on ALL FARMS) ---\n",
    "display(Markdown(\"\\n### Test C: Significance for Binned Continuous Variables (All Farms)\"))\n",
    "def perform_binned_ttest(df, metric_col, metric_name):\n",
    "    df_filtered = df.dropna(subset=[metric_col])\n",
    "    if len(df_filtered) < 4:\n",
    "        print(f\"\\n--- {metric_name} ---\\nNot enough data to perform test.\")\n",
    "        return\n",
    "    threshold = df_filtered[metric_col].median()\n",
    "    low_group = df_filtered[df_filtered[metric_col] < threshold]['trend']\n",
    "    high_group = df_filtered[df_filtered[metric_col] >= threshold]['trend']\n",
    "    if len(low_group) < 2 or len(high_group) < 2:\n",
    "        print(f\"\\n--- {metric_name} ---\\nNot enough data in one of the bins to perform test.\")\n",
    "        return\n",
    "    t_stat, p_val = stats.ttest_ind(low_group, high_group, equal_var=False)\n",
    "    print(f\"\\n--- {metric_name} ---\")\n",
    "    print(f\"Binning Threshold: < {threshold:.2f} (Low, n={len(low_group)}) vs. >= {threshold:.2f} (High, n={len(high_group)})\")\n",
    "    print(f\"  T-statistic: {t_stat:.3f}, P-value: {p_val:.3f}\")\n",
    "    if p_val < 0.05: print(\"  Result: Statistically significant difference found between Low and High groups.\")\n",
    "    else: print(\"  Result: No statistically significant difference between Low and High groups.\")\n",
    "\n",
    "df_test_continuous = representative_df_all_farms\n",
    "perform_binned_ttest(df_test_continuous, 'elevation', 'Mean Site Elevation (km)')\n",
    "perform_binned_ttest(df_test_continuous, 'num_turbines', 'Number of Turbines')\n",
    "perform_binned_ttest(df_test_continuous, 'installed_power_electrical', 'Total Installed Capacity (MWe)')\n",
    "perform_binned_ttest(df_test_continuous, 'capacity_density_MWe_per_sq_km', 'Capacity Density (MWe/km²)')"
   ],
   "id": "9a6c19ef9e53a514",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
